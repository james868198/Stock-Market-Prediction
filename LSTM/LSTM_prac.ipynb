{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train,y_train), (x_test,y_test) = mnist.load_data()\n",
    "print(x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date         Open         High          Low        Close  \\\n",
      "0     2014-02-27  1844.900024  1854.530029  1841.130005  1854.290039   \n",
      "1     2014-02-28  1855.119995  1867.920044  1847.670044  1859.449951   \n",
      "2     2014-03-03  1857.680054  1857.680054  1834.439941  1845.729980   \n",
      "3     2014-03-04  1849.229980  1876.229980  1849.229980  1873.910034   \n",
      "4     2014-03-05  1874.050049  1876.530029  1871.109985  1873.810059   \n",
      "5     2014-03-06  1874.180054  1881.939941  1874.180054  1877.030029   \n",
      "6     2014-03-07  1878.520020  1883.569946  1870.560059  1878.040039   \n",
      "7     2014-03-10  1877.859985  1877.869995  1867.040039  1877.170044   \n",
      "8     2014-03-11  1878.260010  1882.349976  1863.880005  1867.630005   \n",
      "9     2014-03-12  1866.150024  1868.380005  1854.380005  1868.199951   \n",
      "10    2014-03-13  1869.060059  1874.400024  1841.859985  1846.339966   \n",
      "11    2014-03-14  1845.069946  1852.439941  1839.569946  1841.130005   \n",
      "12    2014-03-17  1842.810059  1862.300049  1842.810059  1858.829956   \n",
      "13    2014-03-18  1858.920044  1873.760010  1858.920044  1872.250000   \n",
      "14    2014-03-19  1872.250000  1874.140015  1850.349976  1860.770020   \n",
      "15    2014-03-20  1860.089966  1873.489990  1854.630005  1872.010010   \n",
      "16    2014-03-21  1874.530029  1883.969971  1863.459961  1866.520020   \n",
      "17    2014-03-24  1867.670044  1873.339966  1849.689941  1857.439941   \n",
      "18    2014-03-25  1859.479980  1871.869995  1855.959961  1865.619995   \n",
      "19    2014-03-26  1867.089966  1875.920044  1852.560059  1852.560059   \n",
      "20    2014-03-27  1852.109985  1855.550049  1842.109985  1849.040039   \n",
      "21    2014-03-28  1850.069946  1866.630005  1850.069946  1857.619995   \n",
      "22    2014-03-31  1859.160034  1875.180054  1859.160034  1872.339966   \n",
      "23    2014-04-01  1873.959961  1885.839966  1873.959961  1885.520020   \n",
      "24    2014-04-02  1886.609985  1893.170044  1883.790039  1890.900024   \n",
      "25    2014-04-03  1891.430054  1893.800049  1882.650024  1888.770020   \n",
      "26    2014-04-04  1890.250000  1897.280029  1863.260010  1865.089966   \n",
      "27    2014-04-07  1863.920044  1864.040039  1841.479980  1845.040039   \n",
      "28    2014-04-08  1845.479980  1854.949951  1837.489990  1851.959961   \n",
      "29    2014-04-09  1852.640015  1872.430054  1852.380005  1872.180054   \n",
      "...          ...          ...          ...          ...          ...   \n",
      "1229  2019-01-15  2585.100098  2613.080078  2585.100098  2610.300049   \n",
      "1230  2019-01-16  2614.750000  2625.760010  2612.679932  2616.100098   \n",
      "1231  2019-01-17  2609.280029  2645.060059  2606.360107  2635.959961   \n",
      "1232  2019-01-18  2651.270020  2675.469971  2647.580078  2670.709961   \n",
      "1233  2019-01-22  2657.879883  2657.879883  2617.270020  2632.899902   \n",
      "1234  2019-01-23  2643.479980  2653.189941  2612.860107  2638.699951   \n",
      "1235  2019-01-24  2638.840088  2647.199951  2627.010010  2642.330078   \n",
      "1236  2019-01-25  2657.439941  2672.379883  2657.330078  2664.760010   \n",
      "1237  2019-01-28  2644.969971  2644.969971  2624.060059  2643.850098   \n",
      "1238  2019-01-29  2644.889893  2650.929932  2631.050049  2640.000000   \n",
      "1239  2019-01-30  2653.620117  2690.439941  2648.340088  2681.050049   \n",
      "1240  2019-01-31  2685.489990  2708.949951  2678.649902  2704.100098   \n",
      "1241  2019-02-01  2702.320068  2716.659912  2696.879883  2706.530029   \n",
      "1242  2019-02-04  2706.489990  2724.989990  2698.750000  2724.870117   \n",
      "1243  2019-02-05  2728.340088  2738.979980  2724.030029  2737.699951   \n",
      "1244  2019-02-06  2735.050049  2738.080078  2724.149902  2731.610107   \n",
      "1245  2019-02-07  2717.530029  2719.320068  2687.260010  2706.050049   \n",
      "1246  2019-02-08  2692.360107  2708.070068  2681.830078  2707.879883   \n",
      "1247  2019-02-11  2712.399902  2718.050049  2703.790039  2709.800049   \n",
      "1248  2019-02-12  2722.610107  2748.189941  2722.610107  2744.729980   \n",
      "1249  2019-02-13  2750.300049  2761.850098  2748.629883  2753.030029   \n",
      "1250  2019-02-14  2743.500000  2757.899902  2731.229980  2745.729980   \n",
      "1251  2019-02-15  2760.239990  2775.659912  2760.239990  2775.600098   \n",
      "1252  2019-02-19  2769.280029  2787.330078  2767.290039  2779.760010   \n",
      "1253  2019-02-20  2779.050049  2789.879883  2774.060059  2784.699951   \n",
      "1254  2019-02-21  2780.239990  2781.580078  2764.550049  2774.879883   \n",
      "1255  2019-02-22  2780.669922  2794.199951  2779.110107  2792.669922   \n",
      "1256  2019-02-25  2804.350098  2813.489990  2794.989990  2796.110107   \n",
      "1257  2019-02-26  2792.360107  2803.120117  2789.469971  2793.899902   \n",
      "1258  2019-02-27  2787.500000  2795.760010  2775.129883  2792.379883   \n",
      "\n",
      "        Adj Close      Volume  \n",
      "0     1854.290039  3547460000  \n",
      "1     1859.449951  3917450000  \n",
      "2     1845.729980  3428220000  \n",
      "3     1873.910034  3765770000  \n",
      "4     1873.810059  3392990000  \n",
      "5     1877.030029  3360450000  \n",
      "6     1878.040039  3564740000  \n",
      "7     1877.170044  3021350000  \n",
      "8     1867.630005  3392400000  \n",
      "9     1868.199951  3270860000  \n",
      "10    1846.339966  3670990000  \n",
      "11    1841.130005  3285460000  \n",
      "12    1858.829956  2860490000  \n",
      "13    1872.250000  2930190000  \n",
      "14    1860.770020  3289210000  \n",
      "15    1872.010010  3327540000  \n",
      "16    1866.520020  5270710000  \n",
      "17    1857.439941  3409000000  \n",
      "18    1865.619995  3200560000  \n",
      "19    1852.560059  3480850000  \n",
      "20    1849.040039  3733430000  \n",
      "21    1857.619995  2955520000  \n",
      "22    1872.339966  3274300000  \n",
      "23    1885.520020  3336190000  \n",
      "24    1890.900024  3131660000  \n",
      "25    1888.770020  3055600000  \n",
      "26    1865.089966  3583750000  \n",
      "27    1845.040039  3801540000  \n",
      "28    1851.959961  3721450000  \n",
      "29    1872.180054  3308650000  \n",
      "...           ...         ...  \n",
      "1229  2610.300049  3572330000  \n",
      "1230  2616.100098  3863770000  \n",
      "1231  2635.959961  3772270000  \n",
      "1232  2670.709961  3986730000  \n",
      "1233  2632.899902  3908030000  \n",
      "1234  2638.699951  3335610000  \n",
      "1235  2642.330078  3433250000  \n",
      "1236  2664.760010  3814080000  \n",
      "1237  2643.850098  3612810000  \n",
      "1238  2640.000000  3504200000  \n",
      "1239  2681.050049  3857810000  \n",
      "1240  2704.100098  4917650000  \n",
      "1241  2706.530029  3759270000  \n",
      "1242  2724.870117  3359840000  \n",
      "1243  2737.699951  3560430000  \n",
      "1244  2731.610107  3472690000  \n",
      "1245  2706.050049  4099490000  \n",
      "1246  2707.879883  3622330000  \n",
      "1247  2709.800049  3361970000  \n",
      "1248  2744.729980  3827770000  \n",
      "1249  2753.030029  3670770000  \n",
      "1250  2745.729980  3836700000  \n",
      "1251  2775.600098  3641370000  \n",
      "1252  2779.760010  3533710000  \n",
      "1253  2784.699951  3835450000  \n",
      "1254  2774.879883  3559710000  \n",
      "1255  2792.669922  3427810000  \n",
      "1256  2796.110107  3804380000  \n",
      "1257  2793.899902  3645680000  \n",
      "1258  2792.379883  3767130000  \n",
      "\n",
      "[1259 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sp500 = pd.read_csv('../data/transaction/^GSPC.csv')  \n",
    "print(sp500)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1259\n",
      "998\n",
      "250\n",
      "998\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "train_size = len(sp500)*3//4\n",
    "test_size = len(sp500) - train_size\n",
    "print(len(sp500))\n",
    "# open_train = np.array(sp500['Open'][0:train_size])\n",
    "# close_train = np.array(sp500['Close'][0:train_size])\n",
    "\n",
    "# open_test = np.array(sp500['Open'][train_size:])\n",
    "# cloase_test = np.array(sp500['Close'][train_size:])\n",
    "cl =  np.array(sp500['Close'])\n",
    "max(cl)-min(cl)\n",
    "\n",
    "def processData(data,lb):\n",
    "    X,Y = [],[]\n",
    "    for i in range(len(data)-lb-1):\n",
    "        maxX = max(data[i:(i+lb)])\n",
    "        minX = min(data[i:(i+lb)])\n",
    "        xVal = (2*data[i:(i+lb)] -(maxX+minX))/(maxX-minX)\n",
    "        X.append(xVal)\n",
    "        Y.append(data[(i+lb)])\n",
    "    return np.array(X),np.array(Y)\n",
    "X,y = processData(cl,10)\n",
    "X_train,X_test = X[:int(X.shape[0]*0.80)],X[int(X.shape[0]*0.80):]\n",
    "y_train,y_test = y[:int(y.shape[0]*0.80)],y[int(y.shape[0]*0.80):]\n",
    "\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])\n",
    "print(y_train.shape[0])\n",
    "print(y_test.shape[0])\n",
    "# print(X_train[0])   \n",
    "# print(y_train) \n",
    "# plt.plot(X_train)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "time_steps = 10\n",
    "cell_unit = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4701304 ]\n",
      " [-0.15073067]\n",
      " [-1.        ]\n",
      " [ 0.74435175]\n",
      " [ 0.73816328]\n",
      " [ 0.93748015]\n",
      " [ 1.        ]\n",
      " [ 0.94614711]\n",
      " [ 0.35561653]\n",
      " [ 0.39089631]]\n",
      "Train on 998 samples, validate on 250 samples\n",
      "Epoch 1/300\n",
      "998/998 [==============================] - 4s 4ms/step - loss: 4772685.6142 - val_loss: 7481480.9480\n",
      "Epoch 2/300\n",
      "998/998 [==============================] - 1s 759us/step - loss: 3648525.6170 - val_loss: 1567353.3500\n",
      "Epoch 3/300\n",
      "998/998 [==============================] - 1s 790us/step - loss: 1032192.9223 - val_loss: 358045.8800\n",
      "Epoch 4/300\n",
      "998/998 [==============================] - 1s 732us/step - loss: 605272.5229 - val_loss: 311878.2033\n",
      "Epoch 5/300\n",
      "998/998 [==============================] - 1s 750us/step - loss: 531197.8044 - val_loss: 244815.3764\n",
      "Epoch 6/300\n",
      "998/998 [==============================] - 1s 786us/step - loss: 491296.7621 - val_loss: 228321.0324\n",
      "Epoch 7/300\n",
      "998/998 [==============================] - 1s 778us/step - loss: 453664.8764 - val_loss: 207121.6021\n",
      "Epoch 8/300\n",
      "998/998 [==============================] - 1s 848us/step - loss: 387069.3231 - val_loss: 165979.7009\n",
      "Epoch 9/300\n",
      "998/998 [==============================] - 1s 824us/step - loss: 387169.5346 - val_loss: 175110.3766\n",
      "Epoch 10/300\n",
      "998/998 [==============================] - 1s 802us/step - loss: 360992.6105 - val_loss: 133912.1412\n",
      "Epoch 11/300\n",
      "998/998 [==============================] - 1s 701us/step - loss: 376105.5170 - val_loss: 135969.9535\n",
      "Epoch 12/300\n",
      "998/998 [==============================] - 1s 768us/step - loss: 348634.6630 - val_loss: 117706.2124\n",
      "Epoch 13/300\n",
      "998/998 [==============================] - 1s 803us/step - loss: 368300.3109 - val_loss: 102922.6897\n",
      "Epoch 14/300\n",
      "998/998 [==============================] - 1s 793us/step - loss: 330432.4002 - val_loss: 100827.3166\n",
      "Epoch 15/300\n",
      "998/998 [==============================] - 1s 916us/step - loss: 341773.5740 - val_loss: 99964.2141\n",
      "Epoch 16/300\n",
      "998/998 [==============================] - 1s 849us/step - loss: 331281.8355 - val_loss: 77956.2410\n",
      "Epoch 17/300\n",
      "998/998 [==============================] - 1s 819us/step - loss: 321339.4000 - val_loss: 63206.1029\n",
      "Epoch 18/300\n",
      "998/998 [==============================] - 1s 823us/step - loss: 303828.2491 - val_loss: 65500.3504\n",
      "Epoch 19/300\n",
      "998/998 [==============================] - 1s 847us/step - loss: 295824.7598 - val_loss: 56817.0607\n",
      "Epoch 20/300\n",
      "998/998 [==============================] - 1s 728us/step - loss: 312898.2216 - val_loss: 59586.1408\n",
      "Epoch 21/300\n",
      "998/998 [==============================] - 1s 803us/step - loss: 311367.1794 - val_loss: 57846.1561\n",
      "Epoch 22/300\n",
      "998/998 [==============================] - 1s 776us/step - loss: 300235.3583 - val_loss: 96188.0138\n",
      "Epoch 23/300\n",
      "998/998 [==============================] - 1s 734us/step - loss: 246225.1044 - val_loss: 50432.6653\n",
      "Epoch 24/300\n",
      "998/998 [==============================] - 1s 810us/step - loss: 300615.2454 - val_loss: 42511.7019\n",
      "Epoch 25/300\n",
      "998/998 [==============================] - 1s 823us/step - loss: 307970.9036 - val_loss: 44313.7009\n",
      "Epoch 26/300\n",
      "998/998 [==============================] - 1s 859us/step - loss: 305456.4654 - val_loss: 79600.8932\n",
      "Epoch 27/300\n",
      "998/998 [==============================] - 1s 886us/step - loss: 251599.1741 - val_loss: 41105.8345\n",
      "Epoch 28/300\n",
      "998/998 [==============================] - 1s 919us/step - loss: 291415.0193 - val_loss: 48199.0984\n",
      "Epoch 29/300\n",
      "998/998 [==============================] - 1s 757us/step - loss: 271241.1770 - val_loss: 31798.7409\n",
      "Epoch 30/300\n",
      "998/998 [==============================] - 1s 978us/step - loss: 309673.9237 - val_loss: 29757.8021\n",
      "Epoch 31/300\n",
      "998/998 [==============================] - 1s 1ms/step - loss: 260917.0677 - val_loss: 38802.5617\n",
      "Epoch 32/300\n",
      "998/998 [==============================] - 1s 945us/step - loss: 327525.4096 - val_loss: 33460.1513\n",
      "Epoch 33/300\n",
      "998/998 [==============================] - 1s 791us/step - loss: 291002.2600 - val_loss: 29363.5025\n",
      "Epoch 34/300\n",
      "998/998 [==============================] - 1s 815us/step - loss: 297911.3659 - val_loss: 31571.6108\n",
      "Epoch 35/300\n",
      "998/998 [==============================] - 1s 837us/step - loss: 279071.4001 - val_loss: 32819.5920\n",
      "Epoch 36/300\n",
      "998/998 [==============================] - 1s 808us/step - loss: 289086.3775 - val_loss: 30257.8774\n",
      "Epoch 37/300\n",
      "998/998 [==============================] - 1s 728us/step - loss: 298689.8350 - val_loss: 31702.0036\n",
      "Epoch 38/300\n",
      "998/998 [==============================] - 1s 745us/step - loss: 322266.8585 - val_loss: 24450.3214\n",
      "Epoch 39/300\n",
      "998/998 [==============================] - 1s 842us/step - loss: 282478.1443 - val_loss: 32934.0735\n",
      "Epoch 40/300\n",
      "998/998 [==============================] - 1s 867us/step - loss: 287222.0539 - val_loss: 27585.9878\n",
      "Epoch 41/300\n",
      "998/998 [==============================] - 1s 754us/step - loss: 273399.3776 - val_loss: 21444.1650\n",
      "Epoch 42/300\n",
      "998/998 [==============================] - 1s 826us/step - loss: 324867.8954 - val_loss: 33986.6469\n",
      "Epoch 43/300\n",
      "998/998 [==============================] - 1s 792us/step - loss: 297446.7835 - val_loss: 42263.8204\n",
      "Epoch 44/300\n",
      "998/998 [==============================] - 1s 780us/step - loss: 245724.4491 - val_loss: 35491.1262\n",
      "Epoch 45/300\n",
      "998/998 [==============================] - 1s 775us/step - loss: 255590.5374 - val_loss: 24724.3357\n",
      "Epoch 46/300\n",
      "998/998 [==============================] - 1s 761us/step - loss: 270244.4366 - val_loss: 39194.1200\n",
      "Epoch 47/300\n",
      "998/998 [==============================] - 1s 767us/step - loss: 287679.6588 - val_loss: 35886.9672\n",
      "Epoch 48/300\n",
      "998/998 [==============================] - 1s 832us/step - loss: 252891.3312 - val_loss: 29735.2983\n",
      "Epoch 49/300\n",
      "998/998 [==============================] - 1s 900us/step - loss: 240317.5034 - val_loss: 21378.7339\n",
      "Epoch 50/300\n",
      "998/998 [==============================] - 1s 892us/step - loss: 277234.4508 - val_loss: 24249.0079\n",
      "Epoch 51/300\n",
      "998/998 [==============================] - 1s 821us/step - loss: 259001.9566 - val_loss: 25683.2303\n",
      "Epoch 52/300\n",
      "998/998 [==============================] - 1s 790us/step - loss: 257821.5571 - val_loss: 24937.8739\n",
      "Epoch 53/300\n",
      "998/998 [==============================] - 1s 759us/step - loss: 254304.2080 - val_loss: 21841.6053\n",
      "Epoch 54/300\n",
      "998/998 [==============================] - 1s 812us/step - loss: 269102.0405 - val_loss: 30093.0034\n",
      "Epoch 55/300\n",
      "998/998 [==============================] - 1s 811us/step - loss: 246420.6785 - val_loss: 19516.9990\n",
      "Epoch 56/300\n",
      "998/998 [==============================] - 1s 792us/step - loss: 314791.2143 - val_loss: 21029.0734\n",
      "Epoch 57/300\n",
      "998/998 [==============================] - 1s 832us/step - loss: 288283.3075 - val_loss: 20483.5109\n",
      "Epoch 58/300\n",
      "998/998 [==============================] - 1s 769us/step - loss: 282144.7619 - val_loss: 31042.0678\n",
      "Epoch 59/300\n",
      "998/998 [==============================] - 1s 774us/step - loss: 248785.7890 - val_loss: 17656.8055\n",
      "Epoch 60/300\n",
      "998/998 [==============================] - 1s 933us/step - loss: 276877.0480 - val_loss: 23635.1508\n",
      "Epoch 61/300\n",
      "998/998 [==============================] - 1s 926us/step - loss: 288377.9341 - val_loss: 22052.7017\n",
      "Epoch 62/300\n",
      "998/998 [==============================] - 1s 863us/step - loss: 305653.7710 - val_loss: 33625.3566\n",
      "Epoch 63/300\n",
      "998/998 [==============================] - 1s 756us/step - loss: 242242.5530 - val_loss: 20764.4322\n",
      "Epoch 64/300\n",
      "998/998 [==============================] - 1s 748us/step - loss: 275022.3863 - val_loss: 46973.4797\n",
      "Epoch 65/300\n",
      "998/998 [==============================] - 1s 795us/step - loss: 241501.5161 - val_loss: 17019.0473\n",
      "Epoch 66/300\n",
      "998/998 [==============================] - 1s 754us/step - loss: 305416.3330 - val_loss: 25165.2430\n",
      "Epoch 67/300\n",
      "998/998 [==============================] - 1s 821us/step - loss: 255349.8497 - val_loss: 17017.6558\n",
      "Epoch 68/300\n",
      "998/998 [==============================] - 1s 829us/step - loss: 297657.6232 - val_loss: 17800.8041\n",
      "Epoch 69/300\n",
      "998/998 [==============================] - 1s 798us/step - loss: 281782.8374 - val_loss: 17444.8884\n",
      "Epoch 70/300\n",
      "998/998 [==============================] - 1s 784us/step - loss: 258420.0649 - val_loss: 17078.4755\n",
      "Epoch 71/300\n",
      "998/998 [==============================] - 1s 761us/step - loss: 314805.7037 - val_loss: 21513.1603\n",
      "Epoch 72/300\n",
      "998/998 [==============================] - 1s 753us/step - loss: 270271.3097 - val_loss: 17921.2325\n",
      "Epoch 73/300\n",
      "998/998 [==============================] - 1s 924us/step - loss: 311897.2833 - val_loss: 16412.9750\n",
      "Epoch 74/300\n",
      "998/998 [==============================] - 1s 924us/step - loss: 307301.3014 - val_loss: 15927.3348\n",
      "Epoch 75/300\n",
      "998/998 [==============================] - 1s 875us/step - loss: 288658.3971 - val_loss: 14862.1877\n",
      "Epoch 76/300\n",
      "998/998 [==============================] - 1s 756us/step - loss: 280719.6317 - val_loss: 20102.5950\n",
      "Epoch 77/300\n",
      "998/998 [==============================] - 1s 779us/step - loss: 242074.8553 - val_loss: 15832.3463\n",
      "Epoch 78/300\n",
      "998/998 [==============================] - 1s 748us/step - loss: 281742.6420 - val_loss: 15687.1209\n",
      "Epoch 79/300\n",
      "998/998 [==============================] - 1s 810us/step - loss: 299219.3841 - val_loss: 23342.2422\n",
      "Epoch 80/300\n",
      "998/998 [==============================] - 1s 760us/step - loss: 252931.4647 - val_loss: 17157.7618\n",
      "Epoch 81/300\n",
      "998/998 [==============================] - 1s 797us/step - loss: 282453.4427 - val_loss: 18494.0935\n",
      "Epoch 82/300\n",
      "998/998 [==============================] - 1s 757us/step - loss: 268712.3822 - val_loss: 21015.1433\n",
      "Epoch 83/300\n",
      "998/998 [==============================] - 1s 796us/step - loss: 292938.1691 - val_loss: 12927.4441\n",
      "Epoch 84/300\n",
      "998/998 [==============================] - 1s 816us/step - loss: 287676.3066 - val_loss: 23239.2240\n",
      "Epoch 85/300\n",
      "998/998 [==============================] - 1s 821us/step - loss: 262973.5742 - val_loss: 12676.8449\n",
      "Epoch 86/300\n",
      "998/998 [==============================] - 1s 949us/step - loss: 268194.5772 - val_loss: 17950.8095\n",
      "Epoch 87/300\n",
      "998/998 [==============================] - 1s 1ms/step - loss: 302065.1553 - val_loss: 13775.3249\n",
      "Epoch 88/300\n",
      "998/998 [==============================] - 1s 873us/step - loss: 284182.0897 - val_loss: 15903.0924\n",
      "Epoch 89/300\n",
      "998/998 [==============================] - 1s 805us/step - loss: 273388.6339 - val_loss: 14211.6179\n",
      "Epoch 90/300\n",
      "998/998 [==============================] - 1s 768us/step - loss: 303118.1575 - val_loss: 16338.7298\n",
      "Epoch 91/300\n",
      "998/998 [==============================] - 1s 748us/step - loss: 285807.4872 - val_loss: 18432.1226\n",
      "Epoch 92/300\n",
      "998/998 [==============================] - 1s 743us/step - loss: 293753.3154 - val_loss: 15537.2902\n",
      "Epoch 93/300\n",
      "998/998 [==============================] - 1s 826us/step - loss: 285417.6822 - val_loss: 17296.7850\n",
      "Epoch 94/300\n",
      "998/998 [==============================] - 1s 833us/step - loss: 315759.6350 - val_loss: 18057.2045\n",
      "Epoch 95/300\n",
      "998/998 [==============================] - 1s 761us/step - loss: 261764.4034 - val_loss: 13911.8971\n",
      "Epoch 96/300\n",
      "998/998 [==============================] - 1s 777us/step - loss: 288893.4837 - val_loss: 12416.6774\n",
      "Epoch 97/300\n",
      "998/998 [==============================] - 1s 955us/step - loss: 291860.5198 - val_loss: 17936.9013\n",
      "Epoch 98/300\n",
      "998/998 [==============================] - 1s 944us/step - loss: 313724.9878 - val_loss: 12884.1888\n",
      "Epoch 99/300\n",
      "998/998 [==============================] - 1s 897us/step - loss: 294333.0410 - val_loss: 15006.0644\n",
      "Epoch 100/300\n",
      "998/998 [==============================] - 1s 753us/step - loss: 307897.3330 - val_loss: 14657.9872\n",
      "Epoch 101/300\n",
      "998/998 [==============================] - 1s 751us/step - loss: 273766.0295 - val_loss: 20450.1169\n",
      "Epoch 102/300\n",
      "768/998 [======================>.......] - ETA: 0s - loss: 278908.2266"
     ]
    }
   ],
   "source": [
    "# define/select model\n",
    "model = Sequential() # most common\n",
    "model.add(LSTM(cell_unit, input_shape = (10,1), activation = 'relu', return_sequences = True)) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(cell_unit, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(time_steps, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "apt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "model.compile(optimizer=apt, loss='mse') \n",
    "\n",
    "# print(X_train.shape[0],X_test.shape[1])\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_train[0])\n",
    "history = model.fit(X_train,y_train, epochs = 300, validation_data=(X_test,y_test),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (stock)",
   "language": "python",
   "name": "stock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
